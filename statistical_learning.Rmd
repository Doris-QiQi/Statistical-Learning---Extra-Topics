---
title: "Statistical Learning"
author: "Jingyi Yao"
date: "`r Sys.Date()`"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(glmnet)
set.seed(8105)
```


# LASSO


### Raw Data
```{r}
df <- read_csv("./data/birthweight.csv") 

head(df)
```


\ \par
### Data Cleaning + Sampling


  * `as.factor()` : change dbl into fct, change variable type to use `fct_recode()`
  * `fct_recode(column, "new_value" = "old_value")` change factor level 
  * `as.logical()` change 0 or 1 into FALSE or TRUE
  * `sample_n(sample size)` sample from the cleaned data set

```{r,message=FALSE}
bwt_df = 
  read_csv("./data/birthweight.csv") %>% 
  janitor::clean_names() %>%
  mutate(
    babysex = as.factor(babysex),
    babysex = fct_recode(babysex, "male" = "1", "female" = "2"),
    frace = as.factor(frace),
    frace = fct_recode(frace, "white" = "1", "black" = "2", "asian" = "3", 
                       "puerto rican" = "4", "other" = "8"),
    malform = as.logical(malform),
    mrace = as.factor(mrace),
    mrace = fct_recode(mrace, "white" = "1", "black" = "2", "asian" = "3", 
                       "puerto rican" = "4")) %>% 
  sample_n(200)  # get a sample of size 200

```


\ \par
### Create Design Matrix


  * **glmnet** package predates tidyverse. It needs a design matrix (only contains X)

  * `model.matrix(formula = y ~ x, data = df)` can generate an expanded design matrix
by expanding factors to a set of dummy variables (depending on the contrasts) and expanding interactions similarly.

```{r}
x = model.matrix(bwt ~ ., bwt_df)[,-1]

y = bwt_df$bwt

```


\ \par
### Grid Search for BEST Lambda


  * set lambda as a vector, and fit lasso model for each lambda
  * use cross validation to find the best lambda
  * `cv.glmnet(x, y, lambda, weights, nfolds = 10)` 10-fold CV by default
  * `glmnet(x,y,lambda, alpha = 1)` alpha = 1 by default : LASSO regression

```{r}
lambda = 10^(seq(3, -2, -0.1))      # a vector of lambdas

lasso_fit =
  glmnet(x, y, lambda = lambda,alpha = 1)     # glmnet() with alpha = 1 : lasso

lasso_cv =
  cv.glmnet(x, y, lambda = lambda)  # cross validate lasso model

lasso_cv

lambda_opt = lasso_cv$lambda.min    # the lambda with the minimum MSE
lambda_opt


cvm <- lasso_cv$cvm
cvm

cvsd <- lasso_cv$cvsd
cvsd
```

\ \par

### Coefficients vs. Lambda


  * `complete()` : fill in missing values?
  * `log(lambda, 10)` : take log transformation 
  * `geom_path()` show the lines (trajectory) instead of scattered points
  * `geom_vline(xintercept,color,size)` set color and size for the vertical line at xintercept
  

```{r,warning=FALSE}
broom::tidy(lasso_fit) %>% 
  select(term, lambda, estimate) %>%  # select intercept and other predictors
  complete(term, lambda, fill = list(estimate = 0) ) %>% 
  filter(term != "(Intercept)") %>% 
  ggplot(aes(x = log(lambda, 10), y = estimate, group = term, color = term)) + 
  geom_path(size = .8) + 
  geom_vline(xintercept = log(lambda_opt, 10), color = "blue", size = 1.2)

```


\ \par

### CV Curve

```{r}
broom::tidy(lasso_cv) %>% 
  ggplot(aes(x = log(lambda, 10), y = estimate)) + 
  geom_point()  

```

\ \par

### The OPTIMAL Model


  * The LASSO result does not contain p-values : hard to do inference
  * The estimation is different from OLS result using these selected predictors
  

```{r}
lasso_fit = 
  glmnet(x, y, lambda = lambda_opt)

lasso_fit %>% broom::tidy()

```




# Clustering Analysis : Pokemon


The Pokemon data set contains many variables to describe a Pokemon. 

But we only choose hp and speed here to apply different clustering methods.
```{r}
poke_df = 
  read_csv("./data/pokemon.csv") %>% 
  janitor::clean_names() %>% 
  select(hp, speed)
```


\ \par
A scatter plot showing the relationship between hp and speed.
```{r}
poke_df %>% 
  ggplot(aes(x = hp, y = speed)) + 
  geom_point()

```



### kmeans

  * **kmeans** is a function in **stats** package
  * the raw output of kmeans is a bit messy. It shows the clustering result(number) of each object in the poke_df.
  
```{r}
kmeans_fit =
  kmeans(x = poke_df, centers = 3)

kmeans_fit
```

\ \par

  * using `broom::augment` to add the clustering result to the poke_df (add a column)
  * `broom::augment(kmeans result, df)` add the kmeans result output to the df
  * visualize the clustering result using `ggplot(x,y,color = .cluster)`
  
  
```{r}
poke_df =
  broom::augment(kmeans_fit, poke_df)

poke_df

poke_df %>% 
  ggplot(aes(x = hp, y = speed, color = .cluster)) +
  geom_point()

```


### Choices of Clusters : tuning parameter `centers`

  * k = 2,3,4
  * `map(.x = k, ~ kmeans(df,centers = .x)` using map() to try different centers
  * `map()` result is a listcol so we need to `unnest`
  * `facet_grid(~k)` arrange the plots in k columns
  
```{r}
clusts =
  tibble(k = 2:4) %>%
  mutate(
    km_fit =    map(k, ~kmeans(poke_df, .x)),
    augmented = map(km_fit, ~broom::augment(.x, poke_df))
  )

clusts %>% 
  select(-km_fit) %>% 
  unnest(augmented) %>% 
  ggplot(aes(hp, speed, color = .cluster)) +
  geom_point(aes(color = .cluster)) +
  facet_grid(~k)

```


# Trajectory Clustering
```{r}
traj_data = 
  read_csv("./data/trajectories.csv")

```


```{r}
traj_data %>% 
  ggplot(aes(x = week, y = value, group = subj)) + 
  geom_point() + 
  geom_path()

```


```{r}
int_slope_df = 
  traj_data %>% 
  nest(data = week:value) %>% 
  mutate(
    models = map(data, ~lm(value ~ week, data = .x)),
    result = map(models, broom::tidy)
  ) %>% 
  select(subj, result) %>% 
  unnest(result) %>% 
  select(subj, term, estimate) %>% 
  pivot_wider(
    names_from = term,
    values_from = estimate
  ) %>% 
  rename(int = "(Intercept)", slope = week)

```


```{r}
int_slope_df %>% 
  ggplot(aes(x = int, y = slope)) + 
  geom_point()

```


```{r}
km_fit = 
  kmeans(
    x = int_slope_df %>% select(-subj) %>% scale, 
    centers = 2)

int_slope_df =
  broom::augment(km_fit, int_slope_df)

```


```{r}
int_slope_df %>% 
  ggplot(aes(x = int, y = slope, color = .cluster)) +
  geom_point()

```


```{r}
left_join(traj_data, int_slope_df) %>% 
  ggplot(aes(x = week, y = value, group = subj, color = .cluster)) + 
  geom_point() + 
  geom_path() 

```


```{r}


```


```{r}


```


```{r}


```


```{r}


```


